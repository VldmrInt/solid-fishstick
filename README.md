# Ozon Product Parser

Автоматизированный парсер товаров с Ozon для управления ассортиментом магазина.

## Описание

Проект представляет собой набор скриптов для автоматического сбора и обработки информации о товарах из каталога продавца на Ozon. Система состоит из трех основных компонентов:

1. **script.py** - Многопоточный скрапер для автоматического сбора HTML-страниц
2. **parse_ozon_grok.py** - Парсер HTML для извлечения данных о товарах в XML формат
3. **check_duplicates.py** - Утилита для проверки дубликатов SKU в результатах

## Возможности

- Автоматический сбор страниц каталога Ozon с обходом защиты от ботов
- Многопоточная загрузка страниц (до 20 страниц одновременно)
- Интеллектуальный парсинг с использованием BeautifulSoup и fallback-режимом
- Извлечение названий, SKU и цен товаров
- Экспорт данных в XML формат
- Автоматическая архивация предыдущих результатов
- Проверка дубликатов артикулов
- Подробное логирование всех операций

## Требования

- Python 3.7+
- Google Chrome (для Selenium)

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/VldmrInt/solid-fishstick.git
cd solid-fishstick
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

3. Настройте конфигурацию в `config.json`:
```json
{
  "seller_url": "https://www.ozon.ru/seller/your-shop-name-123456/?miniapp=seller_123456"
}
```

## Использование

### 1. Сбор данных

Запустите основной скрипт для автоматического сбора HTML-страниц и их парсинга:

```bash
python script.py
```

Скрипт автоматически:
- Архивирует старые HTML-файлы
- Загружает все страницы каталога (многопоточно)
- Запускает парсер для обработки собранных данных

### 2. Парсинг HTML (отдельно)

Если у вас уже есть HTML-файлы, можно запустить только парсер:

```bash
python parse_ozon_grok.py
```

### 3. Проверка дубликатов

После парсинга проверьте результаты на наличие дубликатов:

```bash
python check_duplicates.py
```

## Структура проекта

```
solid-fishstick/
├── script.py              # Главный скрипт для сбора HTML
├── parse_ozon_grok.py     # Парсер HTML в XML
├── check_duplicates.py    # Проверка дубликатов SKU
├── config.json            # Конфигурация (URL магазина)
├── requirements.txt       # Зависимости Python
├── .gitignore            # Игнорируемые файлы
├── README.md             # Документация
├── archive/              # Архив старых файлов
│   └── YYYY-MM-DD_HH-MM-SS/
└── parser.log            # Логи работы скриптов
```

## Выходные файлы

### seller_{ID}_output.xml
XML-файл с данными о товарах:
```xml
<?xml version='1.0' encoding='utf-8'?>
<items>
  <item>
    <price1>1 990₽</price1>
    <price2>2 500₽</price2>
    <name>Название товара</name>
    <sku>123456789</sku>
  </item>
  ...
</items>
```

### parse_log.jsonl
Лог пропущенных товаров (JSONL формат):
```json
{"sku": "123456789", "missing": ["price1"], "found": {...}, "sources": [...]}
```

### parser.log
Подробный лог всех операций с временными метками.

## Конфигурация

### config.json
```json
{
  "seller_url": "https://www.ozon.ru/seller/название-магазина-123456/?miniapp=seller_123456"
}
```

### Параметры в script.py
- `GROUP_SIZE = 20` - Количество страниц для параллельной загрузки
- `CHROME_VERSION = 139` - Версия Chrome
- `INITIAL_WAIT = 10` - Начальное ожидание (секунды)
- `MAX_SCROLL_ATTEMPTS = 50` - Максимум попыток скролла

### Параметры в parse_ozon_grok.py
- `SEARCH_WINDOW_SIZE = 4000` - Размер окна поиска
- `MAX_PRICES_PER_ITEM = 2` - Количество цен на товар

## Логирование

Все операции логируются в файл `parser.log`:
- Информация о загрузке страниц
- Количество найденных товаров
- Ошибки парсинга
- Статистика обработки

## Архивация

Старые файлы автоматически архивируются в папку `archive/` с временной меткой:
- HTML-файлы → `archive/YYYY-MM-DD_HH-MM-SS/`
- XML-файлы → `archive/DD-MM-YY_HH_seller_{ID}.xml`

## Устранение проблем

### BeautifulSoup не установлен
```bash
pip install beautifulsoup4 lxml
```

### Ошибки Chrome/Selenium
- Убедитесь, что Chrome установлен
- Проверьте версию Chrome и обновите `CHROME_VERSION` в script.py

### Не находятся товары
- Проверьте правильность URL в config.json
- Убедитесь, что страница доступна
- Проверьте логи в parser.log

## Лицензия

MIT License

## Автор

VldmrInt

## Поддержка

При возникновении проблем создайте issue в репозитории GitHub.